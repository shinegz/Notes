# 计算机基础知识

[字符编码笔记：ASCII，Unicode 和 UTF-8](http://www.ruanyifeng.com/blog/2007/10/ascii_unicode_and_utf-8.html)

[理解字符编码、Unicode、utf-8、utf-16](https://www.letianbiji.com/note/unicode-utf8.html)

## 理解字符编码、Unicode、UTF-8、UTF-16

------

### 字符集、代码点、代码单元

将所有的字符放在一起就是`字符集`。例如将所有的英文字母放在一起可以组成一个字符集，将所有的英文字母和一些标点符号放在一起可以组成一个字符集，将所有的汉字放在一起可以组成一个字符集，将所有的英文字母以及汉字放在一起也可以组成一个字符集。

对一个字符集中的所有字符进行编号（整数的序号），每个字符的编号在这个字符集里都是独一无二的。例如，由所有英文字母组成的字符集中有52个字符（小写字母26个，大写字母26个），可以将这些字符依次编号为0、1、2、...、51。一个字符的编号称为该字符的`代码点（Code Point）`。这种编码后的字符集叫做`编码字符集`。常见的编码字符集有ASCII、Unicode、GBK、UTF8等。

Unicode 本身只给出了字符和字符编号，没有给出存储方式。UTF-8、UTF-16 等编码字符集基于 Unicode 实现，并给出了存储方式。

UTF是"Unicode/UCS Transformation Format"的首字母缩写，即把Unicode字符转换为某种格式之意。

`代码单元（Code Unit）`是编码时使用的最小单元。这个和如何存储一个字符的编码有关。以为下文将提到的UTF-8为例，其代码单元是8bit，由于UTF-8是变长编码，这意味着在UTF-8有些字符用8bit来存储和表示，有些是8×2bit存储和表示，还有8×3bit等等。

### Unicode

Unicode的目标是包含世界上所有语言的文字。Unicode 6.0中已经包含了超过109 000个字符。要了解Unicode，先给出下面的等式：

```javascript
2^16 = 6_5536
17 × 65536 = 111_4112
0x10FFFF = 111_4111
```

Unicode个定义了1 114 112个代码点，值的范围是0～0x10FFFF。这些代码点当前只被使用了一小部分。

Unicode被分为17个区域，一个区域叫一个平面（plane），每个平面有65 536个字符。`0x10FFFF`的最高两位为`0x10`，由0到`0x10`，刚好17个平面，每个平面是16进制表示的最低4位是从`0x0000`到`0xFFFF`。

第一个平面的范围是`0x000000`到`0x00FFFF`，叫做基本多语言平面（BMP），包含了最常用的字符和符号。第2、3、4平面作补充了一些字符。第5到14个平面尚未分配。第15个平面主要包含非非图形化字符。第16、17个平面叫做私有使用区域平面（PUA），供第三方自定义。

Unicode 兼容 ASCII 。

### 如何表示和存储Unicode

一个比较简单的思路是，既然Unicode代码点的值的范围是0～0x10FFF，那么可以直接用3个字节来表示，即使用定长的3个字节来表示所有Unicode代码点对应的字符。但是这存在一个空间使用的问题，例如对于使用英语的人而言，ASCII基本可以满足他的使用。如果使用ASCII，只需要1个字节来存储和表示字符，如果使用Unicode的3个字节来存储，显然是浪费空间的。那么让我们来看看现实中的编码方案。

### UTF-8编码

UTF-8使用8bit作为一个代码单元，是变长编码。先看一下http://en.wikipedia.org/wiki/UTF-8给出的表格。

![img](https://www.letianbiji.com/note/img/utf-8.png)

可以看出，Unicode中代码点可以用7bit表示的字符在UTF-8中均用1字节表示，代码点可以分别用8到11（8、9、10、11）bit表示的字符在UTF-8中均用2字节表示，依次类推。总的来说UTF-8与Unicode兼容，但是可以表示比目前的Unicode更多的字符。

下面举例说明UTF-8与Unicode的映射方式。

汉字“你”的Unicode代码点是`Ox4F60`，其二进制形式（下划线用于优化阅读体验，无特殊意义）：

```javascript
0100_1111 0110_0000
```

至少需要15bit才能表示“你”，所以其对应的UTF-8编码需要3字节：

```JavaScript
1110_0100 10_111101 10_100000
```

即`E4BDA0`。把每个字节`_`后的bit位提取出来放在一起，就是Unicode中对应的代码点了。

### UTF-16

UTF-16使用16bit作为一个代码单元，是变长编码。首先记住这一点：**在Unicode中，代码点0xD800～0xDFFF是没有定义字符的。**

对于Unicode中代码点位于`0x0000`～`0xFFFF`中字符，UTF-16使用一个代码单元即可，那么额对于Unicode中代码点不在`0x0000`～`0xFFFF`之间的字符呢？即对于Unicode中代码点位于`0x10000`～`0x10FFFF`中字符，UTF-16该怎么表示？

**思路是这样的：**使用两个代码单元来表示这些字符，其中第一个代码单元的值在0xD800~0xDBFF之间，第二个代码单元的值在0xDC00~0xDFFF之间，前者称为`高位代理`，后者称为`低位代理`，两者一起称为`代理项对`（surrogate pair）。

**下面计算UTF-16能否表示所有的Unicode字符。**使用一个UTF-16代码单元可以表示的字符数量是`2^16`，`0xD800～0xDFFF`对于Unicode或者UTF-16的一个代理单元都是不可用的。使用两个UTF-16代码单元可以表示的字符数量为：

```JavaScript
(0xdbff-0xd800+1) * (0xdfff-0xdc00+1)
= 1024 * 1024
= 1048576
```

所以UTF-16可以表示的字符数量为：

```javascript
1048576 + 2^16 = 1114112
```

这和Unicode表示的字符数量是相同的。

**对于Unicode中代码点位于`0x10000`～`0x10FFFF`中字符，如何转化为`代理项对`？** 首先，将代码点减去`0x10000`，这样新的代码点的范围变成了`0`～`0xFFFFF`，这些代码点可以用20bit来表示。将这20bit的前10bit（`0～0x3FF`）加上`0xD800`可得到高位代理，将这20bit的后10bit（`0～0x3FF`）加上`0xDC00`可得到低位代理。

对于utf-16还有字节序的问题（即大端序、小端序、混合序），可以参考[维基百科-字节序](http://zh.wikipedia.org/zh-cn/字节序)。

### UTF-32

一个代码单元是32bit，所以一个代码单元就可以表示所有的Unicode字符，是定长编码，与Unicode代码点一一对应。

### UCS-2

定长编码，使用2个字节表示字符，是UTF-16的子集，只能表示Unicode中代码点位于`0x0000`～`0xFFFF`中字符





## 浮点数的二进制表示

http://www.ruanyifeng.com/blog/2010/06/ieee_floating-point_representation.html

1.

前几天，我在读一本C语言教材，有一道例题：

```c
**#include <stdio.h>**

　　**void main(void){**

　　　　**int num=9;** */\* num是整型变量，设为9 \*/*

　　　　**float\* pFloat=#** */\* pFloat表示num的内存地址，但是设为浮点数 \*/*

　　　　**printf("num的值为：%d\n",num);** */\* 显示num的整型值 \*/*

　　　　**printf("\*pFloat的值为：%f\n",\*pFloat);** */\* 显示num的浮点值 \*/*

　　　　***pFloat=9.0;** */\* 将num的值改为浮点数 \*/*

　　　　**printf("num的值为：%d\n",num);** */\* 显示num的整型值 \*/*

　　　　**printf("\*pFloat的值为：%f\n",\*pFloat);** */\* 显示num的浮点值 \*/*

　　**}**
```

运行结果如下：

```bash
**num的值为：9**
　　***pFloat的值为：0.000000**
　　**num的值为：1091567616**
　　***pFloat的值为：9.000000**
```

我很惊讶，num和*pFloat在内存中明明是同一个数，为什么浮点数和整数的解读结果会差别这么大？

要理解这个结果，一定要搞懂浮点数在计算机内部的表示方法。我读了一些资料，下面就是我的笔记。

2.

在讨论浮点数之前，先看一下整数在计算机内部是怎样表示的。

> 　　**int num=9;**

上面这条命令，声明了一个整数变量，类型为int，值为9（二进制写法为1001）。普通的32位计算机，用4个字节表示int变量，所以9就被保存为00000000 00000000 00000000 00001001，写成16进制就是0x00000009。

那么，我们的问题就简化成：**为什么0x00000009还原成浮点数，就成了0.000000？**

3.

根据国际标准IEEE 754，任意一个二进制浮点数V可以表示成下面的形式：

> 　　**![img](http://chart.googleapis.com/chart?cht=tx&chl=V%20%3D%20(-1)%5Es%5Ctimes%20M%5Ctimes%202%5EE&chs=45)**
>
> 　　**（1）(-1)^s表示符号位，当s=0，V为正数；当s=1，V为负数。**
>
> 　　**（2）M表示有效数字，大于等于1，小于2。**
>
> 　　**（3）2^E表示指数位。**

举例来说，十进制的5.0，写成二进制是101.0，相当于1.01×2^2。那么，按照上面V的格式，可以得出s=0，M=1.01，E=2。

十进制的-5.0，写成二进制是-101.0，相当于-1.01×2^2。那么，s=1，M=1.01，E=2。

IEEE 754规定，对于32位的浮点数，最高的1位是符号位s，接着的8位是指数E，剩下的23位为有效数字M。

![img](http://www.ruanyifeng.com/blogimg/asset/201006/bg2010060601.png)

对于64位的浮点数，最高的1位是符号位S，接着的11位是指数E，剩下的52位为有效数字M。

![img](http://www.ruanyifeng.com/blogimg/asset/201006/bg2010060602.png)

5.

IEEE 754对有效数字M和指数E，还有一些特别规定。

前面说过，1≤M<2，也就是说，M可以写成1.xxxxxx的形式，其中xxxxxx表示小数部分。**IEEE 754规定，在计算机内部保存M时，默认这个数的第一位总是1，因此可以被舍去，只保存后面的xxxxxx部分。**比如保存1.01的时候，只保存01，等到读取的时候，再把第一位的1加上去。这样做的目的，是节省1位有效数字。以32位浮点数为例，留给M只有23位，将第一位的1舍去以后，等于可以保存24位有效数字。

至于指数E，情况就比较复杂。

首先，E为一个无符号整数（unsigned int）。这意味着，如果E为8位，它的取值范围为0~255；如果E为11位，它的取值范围为0~2047。但是，我们知道，科学计数法中的E是可以出现负数的，**所以IEEE 754规定，E的真实值必须再减去一个中间数，对于8位的E，这个中间数是127；对于11位的E，这个中间数是1023。**

比如，2^10的E是10，所以保存成32位浮点数时，必须保存成10+127=137，即10001001。

然后，指数E还可以再分成三种情况：

**（1）E不全为0或不全为1。**这时，浮点数就采用上面的规则表示，即指数E的计算值减去127（或1023），得到真实值，再将有效数字M前加上第一位的1。

**（2）E全为0。**这时，浮点数的指数E等于1-127（或者1-1023），有效数字M不再加上第一位的1，而是还原为0.xxxxxx的小数。这样做是为了表示±0，以及接近于0的很小的数字。

**（3）E全为1。**这时，如果有效数字M全为0，表示±无穷大（正负取决于符号位s）；如果有效数字M不全为0，表示这个数不是一个数（NaN）。

6.

好了，关于浮点数的表示规则，就说到这里。

下面，让我们回到一开始的问题：**为什么0x00000009还原成浮点数，就成了0.000000？**

首先，将0x00000009拆分，得到第一位符号位s=0，后面8位的指数E=00000000，最后23位的有效数字M=000 0000 0000 0000 0000 1001。

由于指数E全为0，所以符合上一节的第二种情况。因此，浮点数V就写成：

> 　　V=(-1)^0×0.00000000000000000001001×2^(-126)=1.001×2^(-146)

显然，V是一个很小的接近于0的正数，所以用十进制小数表示就是0.000000。

7.

再看例题的第二部分。

**请问浮点数9.0，如何用二进制表示？还原成十进制又是多少？**

首先，浮点数9.0等于二进制的1001.0，即1.001×2^3。

那么，第一位的符号位s=0，有效数字M等于001后面再加20个0，凑满23位，指数E等于3+127=130，即10000010。

所以，写成二进制形式，应该是s+E+M，即0 10000010 001 0000 0000 0000 0000 0000。这个32位的二进制数，还原成十进制，正是1091567616。