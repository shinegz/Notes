在计算机领域，编码就是将字符映射为二进制序列的过程。编码是一种用来在人与机器之间传递信息的方式，编码的本质是“交流”。

基本概念
● 字符集
字符是各种文字和符号的总称，字符集是多个字符的集合。
● 码点（Code Point，也称代码点）
对一个字符集中的所有字符进行编号（整数序号），编号后的字符集叫做编码字符集，一个字符的编号称为该字符的码点，每个字符的码点在这个编码字符集中是唯一的。
● 码元（Code Unit，也称代码单元）
在将字符映射为二进制序列时使用的最小的比特位数。
ASCII
上个世纪60年代，美国制定了一套字符编码，对英语字符与二进制位之间的关系，做了统一规定。这被称为 ASCII 码，一直沿用至今。ASCII 码一共规定了 128 个字符的编码。
Unicode
世界上存在着多种编码字符集，这意味着同一个编号在不同编码集中可能表示不同的字符。因此，如果我们在解析一个文件时，使用错误的编码字符集去解码，就会出现乱码。
这时，如果有一个编码字符集，能够将世界上所有的字符都囊括其中，每个字符都对应一个唯一编号，那么就不会出现乱码了。
Unicode 就是这样一个编码字符集，它几乎包含了世界上所有的字符，是国际上通用的标准。
Unicode 定义了 1114112 个 码点，值的范围是 0～0x10FFFF。这些码点当前只被使用了一小部分。
2^16 = 65536 
17 × 65536 = 1114112 
0x10FFFF = 1114111 
Unicode 被分为17个区域，一个区域叫一个平面（plane），每个平面有 65536 个字符。0x10FFFF 的最高两位为 0x10，由 0 到 0x10，刚好 17 个平面，每个平面的最低4位是从 0x0000 到 0xFFFF。第一个平面的范围是0x000000 到 0x00FFFF，叫做基本多语言平面（Basic Multilingual Plane, BMP），包含了最常用的文字和符号。基本多语言平面内，从 U+D800 到 U+DFFF 之间的码点区段是永久保留不映射到 Unicode 字符。第 2、3、4 平面作补充了一些字符。第 5 到 14 个平面尚未分配。第 15 个平面主要包含非图形化字符。第 16、17 个平面叫做私有使用区域平面（PUA），供第三方自定义。

Unicode 实现方式
Unicode 的码点值的范围是 0x0000 到 0xFFFF，可以直接用 3 个字节来表示，即使用定长的 3 个字节来表示所有 Unicode 中的所有字符。但对于英文字母而已，只需要使用一个字节表示就够了，会有 2 个字节都是 0，这对存储来说是极大的浪费。因此出现了 Unicode 的多种存储方式，也就是说有多种不同的二进制格式，可以用来表示 Unicode 中的字符的码点。这些方法的名字前面都带有 UTF，UTF 是"Unicode/UCS Transformation Format"的首字母缩写，即把 Unicode 字符码点转换为某种格式之意。
UTF-8
UTF-8 使用 8bit 作为一个码元，是一种变长的编码方式。
码点的位数	码点起值	码点终值	字节序列	Byte 1	Byte 2	Byte 3	Byte 4	Byte 5	Byte 6
  7	U+0000	U+007F	1	0xxxxxxx					
11	U+0080	U+07FF	2	110xxxxx	10xxxxxx				
16	U+0800	U+FFFF	3	1110xxxx	10xxxxxx	10xxxxxx			
21	U+10000	U+1FFFFF	4	11110xxx	10xxxxxx	10xxxxxx	10xxxxxx		
26	U+200000	U+3FFFFFF	5	111110xx	10xxxxxx	10xxxxxx	10xxxxxx	10xxxxxx	
31	U+4000000	U+7FFFFFFF	6	1111110x	10xxxxxx	10xxxxxx	10xxxxxx	10xxxxxx	10xxxxxx
UTF-8 的编码规则很简单，只有二条：
1）对于单字节的符号，字节的第一位设为 0，后面 7 位为这个符号的 Unicode 码。因此对于英语字母，UTF-8 编码和 ASCII 码是相同的。
2）对于 n 字节的符号（n > 1），第一个字节的前 n 位都设为1，第 n + 1 位设为 0，后面字节的前两位一律设为10。剩下的没有提及的二进制位，全部为这个字符的 Unicode 码。

UTF-16
UTF-16 使用 16bit 作为一个码元，是一种变长的编码方式。
16进制编码范围	UTF-16表示方法（二进制）	10进制码范围	字节数量
U+0000 - U+FFFF	xxxx xxxx xxxx xxxx - yyyy yyyy yyyy yyyy	0-65535	2
U+10000 - U+10FFFF	1101 10yy yyyy yyyy - 1101 11xx xxxx xxxx	65536-1114111	4
UTF-16 的编码规则如下：
1）第一个 Unicode 平面（码点从 U+0000 至 U+FFFF）中的字符，UTF-16 使用一个码元即可，码点值等于对应的 Unicode 码点。
2）对于 Unicode 中码点位于 0x10000～0x10FFFF 中的字符，在 UTF-16 中被编码为一对 16 比特长的码元（即 32 位，4 字节），称作代理对（Surrogate Pair），具体方法是：
1. 码点减去 0x10000，得到的值的范围为 20 比特长的 0...0xFFFFF。
2. 高位的 10 比特的值（值的范围为 0...0x3FF）被加上 0xD800 得到第一个码元或称作高位代理（high surrogate），值的范围是 0xD800...0xDBFF。由于高位代理比低位代理的值要小，所以为了避免混淆使用，Unicode 标准现在称高位代理为前导代理（lead surrogates）。
3. 低位的 10 比特的值（值的范围也是 0...0x3FF）被加上 0xDC00 得到第二个码元或称作低位代理（low surrogate），现在值的范围是 0xDC00...0xDFFF。由于低位代理比高位代理的值要大，所以为了避免混淆使用，Unicode 标准现在称低位代理为后尾代理（trail surrogates）。
UTF-16 实现的前提是 Unicode 中 U+D800...U+DFFF 的码点值不对应于任何字符。由于前导代理、后尾代理、BMP 中的有效字符的码点，三者互不重叠，可以通过仅检查一个码元来判定给定字符的下一个字符的起始码元。因此，UTF-16 的解码是简单的，码元值在 U+0000 至 U+FFFF （不包含保留区间）区间的，则该字符对应一个码元，码元值等价于 Unicode 中的码点值；码元值在 U+D800...U+DFFF 区间的，则该字符对应两个码元，两个码元的值与 Unicode 码点值的映射关系如下：
lead \ trail	DC00	DC01	   …   	DFFF
D800	10000	10001	…	103FF
D801	10400	10401	…	107FF
  ⋮	⋮	⋮	⋱	⋮
DBFF	10FC00	10FC01	…	10FFFF
UTF-32
一个码元是 32bit，所以一个码元就可以表示所有的 Unicode 字符，是定长编码，与 Unicode 码点一一对应。
USC-2
定长编码，使用 2 个字节表示字符，是 UTF-16 的子集，只能表示 Unicode 中码点位于 0x0000～0xFFFF 区间中字符。

Web 中的编码知识
URL 编码
URL 表示统一资源定位符（Uniform Resource Locator），每个有效的 URL 都指向 Web 上的一个唯一资源。统一资源定位符的标准格式如下：
[协议类型]://[服务器地址]:[端口号]/[资源路径]?[查询参数]#[片段ID]
其中，端口号（标准 Web 服务器端口号）、查询参数和片段 ID 都是可选的。
因特网标准 RFC 1738 规定，URL 中只允许使用如下字符：
类型	包含
保留字符	; , / ? : @ & = + $
非转义字符	字母 数字 - _ . ! ~ * ' ( )
数字符号	#
所以，当 URL 中出现其他字符时，就需要对其进行编码后使用。Url 编码也称百分号编码，使用 % 百分号加上两位的字符——0123456789ABCDEF——代表一个字节的十六进制形式。对于 Unicode 字 符，RFC 文档建议使用 utf-8 对其进行编码得到相应的字节，然后对每个字节执行百分号编码。如"中文"使用 UTF-8 字符集得到的字节为 0xE4 0xB8 0xAD 0xE6 0x96 0x87，经过 Url 编码之后得到"%E4%B8%AD%E6%96%87"。
ECMAScript 中用于 URL 编码的 API
● encodeURI：编码上述表格外的所有字符
● encodeURIComponent：编码上述表格中非转义字符外的所有字符

Base 64
Base64 是一个基于 64 个可打印字符来表示二进制数据的表示方法，使得二进制数据在解释成 radix-64 的表现形式后能够用 ASCII 字符串的格式表示出来。
由 log264 = 6 可知，每 6 个比特为一个单元，对应某个可打印字符。3 个字节相当于 24 个比特，对应 4 个 Base64 单元。Base64 中的可打印字符包括字母 A-Z、a-z、数字 0-9，这样共有 62 个字符，此外两个可打印字符在不同的系统中而不同，其编码表如下图所示：
十进制	二进制	字符	 	十进制	二进制	字符	 	十进制	二进制	字符	 	十进制	二进制	字符
0	000000	A	16	010000	Q	32	100000	g	48	110000	w
1	000001	B	17	010001	R	33	100001	h	49	110001	x
2	000010	C	18	010010	S	34	100010	i	50	110010	y
3	000011	D	19	010011	T	35	100011	j	51	110011	z
4	000100	E	20	010100	U	36	100100	k	52	110100	0
5	000101	F	21	010101	V	37	100101	l	53	110101	1
6	000110	G	22	010110	W	38	100110	m	54	110110	2
7	000111	H	23	010111	X	39	100111	n	55	110111	3
8	001000	I	24	011000	Y	40	101000	o	56	111000	4
9	001001	J	25	011001	Z	41	101001	p	57	111001	5
10	001010	K	26	011010	a	42	101010	q	58	111010	6
11	001011	L	27	011011	b	43	101011	r	59	111011	7
12	001100	M	28	011100	c	44	101100	s	60	111100	8
13	001101	N	29	011101	d	45	101101	t	61	111101	9
14	001110	O	30	011110	e	46	101110	u	62	111110	+
15	001111	P	31	011111	f	47	101111	v	63	111111	/
如果要编码的字节数不能被 3 整除，最后会多出 1 个或 2 个字节，那么可以使用下面的方法进行处理：先使用 0 字节值在末尾补足，使其能够被 3 整除，然后再进行 Base64 的编码。在编码后的 Base64 文本后加上一个或两个= 号，代表补足的字节数。也就是说：
● 当最后剩余两个八位(待补足)字节（2 个 byte）时，最后一个 6 位的 Base64 字节块有四位是 0 值，最后附加上两个等号。
● 如果最后剩余一个八位(待补足)字节（1 个 byte）时，最后一个 6 位的 Base64 字节块有两位是 0 值，最后附加一个等号。 
文本（1 Byte）	A		
二进制位	0	1	0	0	0	0	0	1	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0
二进制位（补0）	0	1	0	0	0	0	0	1	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0	0
Base64编码	Q	Q	=	=
文本（2 Byte）	B	C	
二进制位	0	1	0	0	0	0	1	0	0	1	0	0	0	0	1	1	0	0	0	0	0	0	0	0
二进制位（补0）	0	1	0	0	0	0	1	0	0	1	0	0	0	0	1	1	0	0	0	0	0	0	0	0
Base64编码	Q	k	M	=
ECMAScript 中用于 Base64 编码的 API
● btoa：将“二进制”字符串编码为 ASCII 字符串
● atob：将 ASCII 字符串解码为“二进制”字符串

参考资料
http://www.ruanyifeng.com/blog/2007/10/ascii_unicode_and_utf-8.html
https://www.letianbiji.com/note/unicode-utf8.html
https://zh.m.wikipedia.org/zh/Unicode
https://zh.wikipedia.org/zh-cn/UTF-16
